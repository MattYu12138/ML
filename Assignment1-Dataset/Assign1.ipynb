{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OigpLIHWTQU5"
      },
      "source": [
        "# COMP5329 - Deep Learning\n",
        "## Tutorial 2 - Multilayer Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9Dx7C_TQU6"
      },
      "source": [
        "**Semester 1, 2022**\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "* To understand the multi-layer perceptron.\n",
        "* To become familiar with backpropagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxMRrQEBTQU7"
      },
      "source": [
        "## Loading the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PiIktNG2TQU8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "from matplotlib import animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of tran_data is (50000, 128) and of tran_label is (50000, 1)\n",
            "shape of test_data is (10000, 128) and of test_label is (10000, 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Minimum': -23.41520966868848,\n",
              " 'Maximum': 25.581360537720347,\n",
              " 'Mean': -1.593740250882547e-14,\n",
              " 'Standard Deviation': 1.1688636581970187,\n",
              " 'label example': array([[6],\n",
              "        [9],\n",
              "        [9],\n",
              "        [4],\n",
              "        [1],\n",
              "        [1],\n",
              "        [2],\n",
              "        [7],\n",
              "        [8],\n",
              "        [3]], dtype=uint8)}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = np.load('train_data.npy')\n",
        "tr_label = np.load('train_label.npy')\n",
        "test_data = np.load('test_data.npy')\n",
        "ts_label = np.load('test_label.npy')\n",
        "\n",
        "print(\"shape of tran_data is {} and of tran_label is {}\".format(train_data.shape, tr_label.shape))\n",
        "print(\"shape of test_data is {} and of test_label is {}\".format(test_data.shape, ts_label.shape))\n",
        "\n",
        "data_stats = {\n",
        "    'Minimum': np.min(train_data),\n",
        "    'Maximum': np.max(train_data),\n",
        "    'Mean': np.mean(train_data),\n",
        "    'Standard Deviation': np.std(train_data),\n",
        "    \"label example\" : tr_label[:10]\n",
        "}\n",
        "\n",
        "data_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-processing\n",
        "\n",
        "Obviously, the mean and deviation of training datasets are really close to 0 and 1 respectively, which means that the data can be used directly as a normalized data. However this time I still choose to process normalization for precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-1.4710455076283324e-19, 1.0000000000000002)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_data_standardized = scaler.fit_transform(train_data)\n",
        "test\n",
        "\n",
        "standardized_mean = np.mean(train_data_standardized)\n",
        "standardized_std = np.std(train_data_standardized)\n",
        "\n",
        "standardized_mean, standardized_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convert_to_one_hot(labels, num_classes):\n",
        "    # Create an array of zeros with shape (num_samples, num_classes)\n",
        "    one_hot_labels = np.zeros((labels.size, num_classes))\n",
        "    \n",
        "    # Use numpy advanced indexing to set the appropriate elements to one\n",
        "    one_hot_labels[np.arange(labels.size), labels.flatten()] = 1\n",
        "    \n",
        "    return one_hot_labels\n",
        "\n",
        "train_label = convert_to_one_hot(tr_label, 10)\n",
        "test_label = convert_to_one_hot(ts_label, 10)\n",
        "\n",
        "train_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBbVI6fyTQVI"
      },
      "source": [
        "## Definition of some activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ZooR-OTQVK"
      },
      "source": [
        "Linear\n",
        "$$output = x$$\n",
        "\n",
        "Tanh  \n",
        "$$output = tanh(x)$$  \n",
        "\n",
        "Sigmoid\n",
        "$$output = \\frac {1}{1 + e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "csIF2JfSTQVK"
      },
      "outputs": [],
      "source": [
        "# create a activation class\n",
        "# for each time, we can initiale a activation function object with one specific function\n",
        "# for example: f = Activation(\"tanh\")  means we create a tanh activation function.\n",
        "# you can define more activation functions by yourself, such as relu!\n",
        "\n",
        "class Activation(object):\n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "    def __tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)\n",
        "        return 1.0 - a**2\n",
        "    \n",
        "    def __logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    def __logistic_deriv(self, a):\n",
        "        # a = logistic(x)\n",
        "        return  a * (1 - a )\n",
        "    \n",
        "    def __ReLU(self, x):\n",
        "        return np.maximum(0,x)\n",
        "    def __ReLU_deriv(self, a):\n",
        "        #a = ReLU(x)\n",
        "        return np.where(a > 0, 1, 0)\n",
        "    \n",
        "    def __softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "    def __softmax_deriv(self, x):\n",
        "       \n",
        "        pass\n",
        "\n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv\n",
        "        elif activation == 'ReLU':\n",
        "            self.f = self.__ReLU\n",
        "            self.f_deriv = self.__ReLU_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91dBdVq4TQVN"
      },
      "source": [
        "### Define HiddenLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBnDirODTQVO"
      },
      "source": [
        "$$output = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W_{i})} + b)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GymZcsO0TQVO"
      },
      "outputs": [],
      "source": [
        "# now we define the hidden layer for the mlp\n",
        "# for example, h1 = HiddenLayer(10, 5, activation=\"tanh\") means we create a layer with 10 dimension input and 5 dimension output, and using tanh activation function.\n",
        "# notes: make sure the input size of hiddle layer should be matched with the output size of the previous layer!\n",
        "\n",
        "class HiddenLayer(object):\n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='softmax',activation='ReLU', W=None, b=None):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.activation=Activation(activation).f \n",
        "\n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        # we randomly assign small values for the weights as the initiallization\n",
        "        # self.W = np.random.uniform(\n",
        "        #         low=-np.sqrt(6. / (n_in + n_out)),\n",
        "        #         high=np.sqrt(6. / (n_in + n_out)),\n",
        "        #         size=(n_in, n_out)\n",
        "        # )\n",
        "        # if activation == 'logistic':\n",
        "        #     self.W *= 4\n",
        "            \n",
        "        # if activation == 'relu':\n",
        "        # apply HE initialization\n",
        "        self.W = np.random.randn(n_in, n_out) * np.sqrt(2. /n_in)\n",
        "\n",
        "        # we set the size of bias as the size of output dimension\n",
        "        self.b = np.zeros(n_out,)\n",
        "\n",
        "        # we set he size of weight gradation as the size of weight\n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "\n",
        "\n",
        "    # the forward and backward progress (in the hidden layer level) for each training epoch\n",
        "    # please learn the week2 lec contents carefully to understand these codes.\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, delta, output_layer=False):\n",
        "\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        self.grad_b = delta\n",
        "        if self.activation_deriv is not None:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        else:\n",
        "            delta = delta.dot(self.W.T)\n",
        "        return delta\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwsUH3fhTQVR"
      },
      "source": [
        "## The MLP\n",
        "\n",
        "The class implements a MLP with a fully configurable number of layers and neurons. It adapts its weights using the backpropagation algorithm in an online manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QWpvH41iTQVR"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    # for initiallization, the code will create all layers automatically based on the provided parameters.\n",
        "    def __init__(self, layers, activation=[None,'softmax','softmax']):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        \"\"\"\n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params=[]\n",
        "\n",
        "        self.activation=activation\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1]))\n",
        "\n",
        "    # forward progress: pass the information through the layers and out the results of final output layer\n",
        "    def forward(self,input):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input)\n",
        "            input=output\n",
        "        return output\n",
        "\n",
        "    # define the objection/loss function, we use mean sqaure error (MSE) as the loss\n",
        "    # you can try other loss, such as cross entropy.\n",
        "    # when you try to change the loss, you should also consider the backward formula for the new loss as well!\n",
        "    def criterion_MSE(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # MSE\n",
        "        error = y-y_hat\n",
        "        loss=error**2\n",
        "        # calculate the MSE's delta of the output layer\n",
        "        delta=-2*error*activation_deriv(y_hat)\n",
        "        # return loss and delta\n",
        "        return loss,delta\n",
        "    \n",
        "    def Cross_Entropy(self,y,y_hat, epsilon = 1e-12):\n",
        "        # Ensure the predictions are within the range (epsilon, 1-epsilon) for numerical stability\n",
        "        y_hat = np.clip(y_hat, epsilon, 1. - epsilon)\n",
        "        # Compute cross-entropy loss for each class and sum\n",
        "        cross_entropy = -np.sum(y * np.log(y_hat))\n",
        "        # Calculate the gradient for the output layer\n",
        "        delta = y_hat - y\n",
        "        return cross_entropy, delta\n",
        "                        \n",
        "    # def update_adam(self,X,y,learning_rate = 0.1, epochs = 100, epsilon = 1e-12):\n",
        "    #     for layer in self.layers:\n",
        "    #         layer.W = beta1\n",
        "\n",
        "            \n",
        "    # backward progress\n",
        "    def backward(self,delta):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta)\n",
        "\n",
        "    # update the network weights after backward.\n",
        "    # make sure you run the backward function before the update function!\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    def update(self,lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "\n",
        "    # define the training function\n",
        "    # it will return all losses within the whole training process.\n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\"\n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "\n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            for it in range(X.shape[0]):\n",
        "                i=np.random.randint(X.shape[0])\n",
        "\n",
        "                # forward pass\n",
        "                y_hat = self.forward(X[i])\n",
        "\n",
        "                # backward pass\n",
        "                # loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
        "                loss[it],delta=self.Cross_Entropy(y[i],y_hat)\n",
        "                self.backward(delta)\n",
        "                y\n",
        "                # update\n",
        "                self.update(learning_rate)\n",
        "            to_return[k] = np.mean(loss)\n",
        "        return to_return\n",
        "\n",
        "    # define the prediction function\n",
        "    # we can use predict function to predict the results of new data, by using the well-trained network.\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = np.zeros(x.shape[0])\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output[i] = self.forward(x[i,:])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlgcO0CNTQVU"
      },
      "source": [
        "## Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iu7RlvZYTQVV"
      },
      "outputs": [],
      "source": [
        "# ### Try different MLP models\n",
        "# # we can compare the loss change graph to under how the network parameters (such as number of layers and activation functions),\n",
        "# # could affect the performance of network.\n",
        "# nn = MLP([2,4,8,1], [None,'ReLU','ReLU','ReLU'])\n",
        "# input_data = train_data[:,0:2]\n",
        "# output_data = train_data[:,2]\n",
        "# MSE = nn.fit(input_data, output_data, learning_rate=0.001, epochs=50)\n",
        "# print('loss:%f'%MSE[-1])\n",
        "# pl.figure(figsize=(15,4))\n",
        "# pl.plot(MSE)\n",
        "# pl.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Try different MLP models\n",
        "# we can compare the loss change graph to under how the network parameters (such as number of layers and activation functions),\n",
        "# could affect the performance of network.\n",
        "nn = MLP([2,4,4,10], [None,'ReLU','ReLU','softmax'])\n",
        "input_data = train_data[:,0:2]\n",
        "output_data = train_data[:,2]\n",
        "CE = nn.fit(input_data, output_data, learning_rate=0.1, epochs=50)\n",
        "print('loss:%f'%CE[-1])\n",
        "pl.figure(figsize=(15,4))\n",
        "pl.plot(CE)\n",
        "pl.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN93ZYW3TQVc"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpAUm5T7TQVd"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 110\u001b[0m, in \u001b[0;36mMLP.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 110\u001b[0m     \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x[i,:])\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "output = nn.predict(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "Y9DTHUbXTQVg",
        "outputId": "95b2e6c7-f6bf-4faf-859b-5b323fde9a0c"
      },
      "outputs": [],
      "source": [
        "# visualizing the predict results\n",
        "# notes: since we use tanh function for the final layer, that means the output will be in range of [0,1]\n",
        "pl.figure(figsize=(8,6))\n",
        "pl.scatter(output_data, output, s=100)\n",
        "pl.xlabel('Targets')\n",
        "pl.ylabel('MLP output')\n",
        "pl.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "3OR0tt8fTQVi",
        "outputId": "b78ff76c-3998-4d56-c653-cfbeca94840e"
      },
      "outputs": [],
      "source": [
        "# create a mesh to plot in\n",
        "xx, yy = np.meshgrid(np.arange(-2, 2, .02),np.arange(-2, 2, .02))\n",
        "\n",
        "# Plot the decision boundary. For that, we will assign a color to each\n",
        "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
        "Z = nn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "pl.figure(figsize=(15,7))\n",
        "pl.subplot(1,2,1)\n",
        "pl.pcolormesh(xx, yy, Z>0, cmap='cool')\n",
        "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[d>0] for d in output_data], s=100)\n",
        "pl.xlim(-2, 2)\n",
        "pl.ylim(-2, 2)\n",
        "pl.grid()\n",
        "pl.title('Targets')\n",
        "pl.subplot(1,2,2)\n",
        "pl.pcolormesh(xx, yy, Z>0, cmap='cool')\n",
        "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[d>0] for d in output], s=100)\n",
        "pl.xlim(-2, 2)\n",
        "pl.ylim(-2, 2)\n",
        "pl.grid()\n",
        "pl.title('MLP output')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytqTFdkQfobY"
      },
      "source": [
        "### the figure on the left shows the ground true label of each data\n",
        "### the figure on the eright shows the predict label of each data with MLP model.\n",
        "### Based on the visualization result, we can find that network learned a boundary between positive and negative data!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
